# /pages/12_ğŸ¤–_AI_Assistant.py

import streamlit as st
import pandas as pd
import requests
from openai import OpenAI
import os

# --- é¡µé¢è®¾ç½® ---
st.set_page_config(
    page_title="AI æ™ºèƒ½åˆ†æåŠ©æ‰‹",
    page_icon="ğŸ¤–",
    layout="wide"
)

# --- å¯¼å…¥å¹¶åº”ç”¨èƒŒæ™¯æ ·å¼ ---
from utils import style

style.set_page_background('assets/backgroud.png')

# --- åç«¯æœåŠ¡é…ç½® ---
# ä¸» Agent æœåŠ¡åœ°å€ï¼Œç”¨äºæ–‡ä»¶å¤„ç†
AGENT_BASE_URL = "http://127.0.0.1:8000"  # ä½¿ç”¨ 127.0.0.1 è€Œä¸æ˜¯ 0.0.0.0
FILE_ANALYSIS_ENDPOINT = f"{AGENT_BASE_URL}/analyze_reviews/"

# vLLM OpenAI é£æ ¼ API åœ°å€ï¼Œç”¨äºå¯¹è¯
VLLM_BASE_URL = "http://hpc.wisesoe.com:58001/v1"
VLLM_MODEL_NAME = "deepseek-r1-distill-qwen-vllm"


# --- ä¸åç«¯ AI Agent äº¤äº’çš„å‡½æ•° ---

def analyze_file_with_agent(uploaded_file):
    """
    é€šè¿‡ HTTP è¯·æ±‚å°†æ–‡ä»¶å‘é€åˆ°ä¸» Agent æœåŠ¡è¿›è¡Œå¤„ç†ã€‚
    æœŸæœ›åç«¯è¿”å›ä¸€ä¸ª JSONï¼ŒåŒ…å«å¤„ç†åçš„æ•°æ®å’Œåˆ†æå»ºè®®ã€‚
    """
    try:
        files = {'file': (uploaded_file.name, uploaded_file.getvalue(), uploaded_file.type)}
        response = requests.post(FILE_ANALYSIS_ENDPOINT, files=files, timeout=300)  # è®¾ç½®è¾ƒé•¿çš„è¶…æ—¶

        if response.status_code == 200:
            result = response.json()
            # å‡è®¾åç«¯è¿”å›æ ¼å¼ä¸º: {"structured_data": [...], "suggestions": "..."}
            processed_df = pd.DataFrame(result.get("structured_data"))
            suggestions_text = result.get("suggestions")
            return processed_df, suggestions_text
        else:
            error_message = f"æ–‡ä»¶åˆ†æå¤±è´¥ã€‚æœåŠ¡å™¨è¿”å›çŠ¶æ€ç : {response.status_code}ã€‚é”™è¯¯ä¿¡æ¯: {response.text}"
            return None, error_message
    except requests.exceptions.RequestException as e:
        error_message = f"æ— æ³•è¿æ¥åˆ°åˆ†ææœåŠ¡ï¼Œè¯·ç¡®è®¤AI Agentä¸»æœåŠ¡æ­£åœ¨ {AGENT_BASE_URL} è¿è¡Œã€‚é”™è¯¯è¯¦æƒ…: {e}"
        return None, error_message


# --- ä¸ vLLM æœåŠ¡äº¤äº’çš„å‡½æ•° ---

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ŒæŒ‡å‘æ‚¨çš„ vLLM æœåŠ¡
try:
    client = OpenAI(
        base_url=VLLM_BASE_URL,
        api_key="not-needed"  # å¯¹äºæœ¬åœ°æˆ–ç§æœ‰éƒ¨ç½²çš„æœåŠ¡ï¼ŒAPIå¯†é’¥é€šå¸¸ä¸æ˜¯å¿…éœ€çš„
    )
except Exception as e:
    st.error(f"åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯å¤±è´¥: {e}")
    client = None


def get_chat_response(messages):
    """
    ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯ä¸æ‚¨çš„ vLLM æ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚
    """
    if not client:
        return "é”™è¯¯ï¼šæ— æ³•ä¸AIå¯¹è¯æœåŠ¡å»ºç«‹è¿æ¥ã€‚"
    try:
        response = client.chat.completions.create(
            model=VLLM_MODEL_NAME,
            messages=messages,
            temperature=0.7,
        )
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"ä¸AIå¯¹è¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return "æŠ±æ­‰ï¼Œæˆ‘åœ¨å›ç­”æ—¶é‡åˆ°äº†ä¸€ä¸ªé—®é¢˜ã€‚"


# --- Streamlit é¡µé¢ UI ---

st.title("ğŸ¤– æ™ºèƒ½èˆ†æƒ…åˆ†æåŠ©æ‰‹")
st.markdown(
    "ä¸Šä¼ æ‚¨çš„åŸå§‹è¯„è®ºæ–‡ä»¶ï¼ˆCSVæˆ–Excelï¼‰ï¼ŒAIå°†è°ƒç”¨BERTæ¨¡å‹è¿›è¡Œç»“æ„åŒ–å¤„ç†ï¼Œå¹¶ç”±å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Šã€‚éšåï¼Œæ‚¨å¯å°±æŠ¥å‘Šå†…å®¹ä¸AIè¿›è¡Œå¯¹è¯ã€‚")
st.markdown("---")

# --- åˆå§‹åŒ– Session State ---
if "analysis_report" not in st.session_state:
    st.session_state.analysis_report = None
if "structured_data" not in st.session_state:
    st.session_state.structured_data = None
if "chat_messages" not in st.session_state:
    st.session_state.chat_messages = []
if "file_processed" not in st.session_state:
    st.session_state.file_processed = False

# --- 1. æ–‡ä»¶ä¸Šä¼ ä¸åˆ†æ ---
with st.container(border=True):
    st.subheader("ç¬¬ä¸€æ­¥ï¼šä¸Šä¼ æ–‡ä»¶å¹¶å¯åŠ¨åˆ†æ")
    uploaded_file = st.file_uploader(
        "æ”¯æŒCSVæˆ–Excelæ ¼å¼çš„è¯„è®ºæ–‡ä»¶",
        type=['csv', 'xlsx']
    )

    if uploaded_file is not None:
        if st.button("ğŸš€ å¼€å§‹æ™ºèƒ½åˆ†æ", type="primary", use_container_width=True):
            with st.spinner("è¯·ç¨å€™... AI Agentæ­£åœ¨è°ƒç”¨BERTæ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æ..."):
                df, report = analyze_file_with_agent(uploaded_file)

            if df is not None:
                st.success("æ–‡ä»¶åˆ†æå®Œæˆï¼")
                st.session_state.structured_data = df
                st.session_state.analysis_report = report
                st.session_state.file_processed = True
                # æ¸…ç©ºæ—§çš„å¯¹è¯å†å²å¹¶æ·»åŠ æ–°çš„ç³»ç»Ÿæç¤º
                st.session_state.chat_messages = [
                    {"role": "assistant",
                     "content": "æ‚¨å¥½ï¼æˆ‘å·²ç»åˆ†æå®Œæ‚¨ä¸Šä¼ çš„æ–‡ä»¶ã€‚è¯·æŸ¥çœ‹ä¸‹æ–¹çš„æŠ¥å‘Šå’Œæ•°æ®ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥å¼€å§‹å¯¹è¯ã€‚"}
                ]
            else:
                st.error(report)  # å¦‚æœå¤±è´¥ï¼Œreportå˜é‡ä¼šåŒ…å«é”™è¯¯ä¿¡æ¯
                st.session_state.file_processed = False

# --- 2. åˆ†æç»“æœå±•ç¤ºä¸å¯¹è¯ ---
if st.session_state.file_processed:
    st.markdown("---")
    st.subheader("ç¬¬äºŒæ­¥ï¼šæŸ¥çœ‹åˆ†ææŠ¥å‘Šå¹¶å¼€å§‹å¯¹è¯")

    # å±•ç¤ºåˆ†ææŠ¥å‘Šå’Œç»“æ„åŒ–æ•°æ®
    with st.expander("ç‚¹å‡»æŸ¥çœ‹AIç”Ÿæˆçš„åˆ†ææŠ¥å‘Š", expanded=True):
        st.markdown(st.session_state.analysis_report)

    with st.expander("ç‚¹å‡»æŸ¥çœ‹ç»“æ„åŒ–æ•°æ®è¯¦æƒ…"):
        st.dataframe(st.session_state.structured_data, use_container_width=True)

    # å¯¹è¯ç•Œé¢
    st.markdown("#### ä¸AIå¯¹è¯")

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.chat_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # æ¥æ”¶ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("å°±åˆ†ææŠ¥å‘Šæé—®ï¼Œè·å–æ›´æ·±å…¥çš„è§è§£..."):
        # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å†å²è®°å½•
        st.session_state.chat_messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # å‡†å¤‡å‘é€ç»™ vLLM çš„æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«ç³»ç»Ÿçº§ä¸Šä¸‹æ–‡
        messages_for_vllm = [
                                {
                                    "role": "system",
                                    "content": f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™¯åŒºèˆ†æƒ…åˆ†æå¸ˆã€‚ä½ å·²ç»åˆ†æäº†ä¸€ä»½è¯„è®ºæ•°æ®ï¼Œå¹¶ç”Ÿæˆäº†ä»¥ä¸‹çš„åˆ†ææŠ¥å‘Šï¼š\n\n---æŠ¥å‘Šå¼€å§‹---\n{st.session_state.analysis_report}\n---æŠ¥å‘Šç»“æŸ---\n\nç°åœ¨ï¼Œè¯·æ ¹æ®è¿™ä»½æŠ¥å‘Šå’Œä½ çš„ä¸“ä¸šçŸ¥è¯†ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
                                }
                            ] + st.session_state.chat_messages

        # è·å– AI çš„å“åº”
        with st.chat_message("assistant"):
            with st.spinner("AI æ­£åœ¨æ€è€ƒ..."):
                response = get_chat_response(messages_for_vllm)
                st.markdown(response)

        # å°† AI çš„å“åº”ä¹Ÿæ·»åŠ åˆ°å†å²è®°å½•
        st.session_state.chat_messages.append({"role": "assistant", "content": response})
else:
    st.info("è¯·å…ˆä¸Šä¼ æ–‡ä»¶å¹¶å®Œæˆåˆ†æï¼Œä»¥ä¾¿å¯ç”¨å¯¹è¯åŠŸèƒ½ã€‚")